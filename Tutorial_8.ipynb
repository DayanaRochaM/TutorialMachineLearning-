{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Regressão Linear Multivariada**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os algoritmos de otimização são usados por algoritmos de aprendizado de máquina para encontrar um bom conjunto de parâmetros do modelo, dado um conjunto de dados de treinamento. O Gradiente Descendente Estocástico é o algoritmo de otimização mais comum usado em aprendizado de máquina. Neste tutorial vamos descobrir, com o Python, como implementar esse algoritmo em conjunto com um algoritmo de Regressão Linear. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Antes de vermos as implementações, vamos entender alguns conceitos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Regressão Linear Multivariada**\n",
    "<br />\n",
    "É um algoritmo de aprendizado de máquina que é responsável por descobrir um conjunto de coeficientes b0 á bn tal que<br />\n",
    "<br />\n",
    "<center><strong>y = b0 + b1 \\* x1 + b2 \\* x2 + ... + bn \\* xn</strong> ,</center>\n",
    "\n",
    "onde cada atributo x é um input e y é uma previsão.<br />\n",
    "Nesse tutorial, os coeficientes serão encontrados através do Gradiente Descendente Estocástico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Gradiente Descendente Estocástico\n",
    "Processo de minização de uma função seguindo a inclinação ou gradiente da função. No aprendizado de máquina, podemos\n",
    "usar uma técnica que avalia e atualiza os coeficientes a cada iteração para minimizar o erro de um modelo ou \n",
    "dados de treinamento. O modelo faz uma previsão para um treinamento de instância, o erro é calculado e o modelo é \n",
    "atualizado para reduzir o erro na próxima previsão. Essa operação é repetida para um número fixo de iterações.\n",
    "\n",
    "A cada iteração, o valor de b é atualizado para\n",
    "<center><strong> b = b - taxa de aprendizado \\* erro \\* x </strong> ,</center>\n",
    "\n",
    "onde b é o coeficiente sendo otimizado, taxa de aprendizado é um valor que pode e deve ser fornecido de acordo com\n",
    "a necessidade, erro é o erro da previsão do modelo sobre os dados de treinamento e x é um valor de entrada.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br />\n",
    "Neste tutorial usaremos um conjunto de dados sobre qualidade do vinho branco. Nosso exemplo, no final do tutorial, buscará prever a qualidade do vinho branco. Baixe o conjunto de dados através do link https://github.com/DayanaRochaM/TutorialMachineLearning-/blob/master/winequality-white.csv e guarde-o no seu diretório de trabalho atual sem alterar seu nome (<strong>winequality-white.csv</strong>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passos do tutorial:\n",
    " 1. Fazer previsões: implementar algoritmo que faz previsões e testá-lo com coeficientes previamente fornecidos.\n",
    " 2. Estimar coeficientes: implementar algoritmo que estima os coeficientes a serem usados nas previsões (Regressão Linear usando Gradiente Descendente Estocástico).\n",
    " 3. Estudo de caso do vinho de qualidade: testar algoritmos implementados em um dataset sobre a qualidade do vinho branco."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Fazer previsões"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O primeiro passo é desenvolver uma função que faz previsões. Esta vai ser necessária para a avaliação dos possíveis valores de coeficientes. Abaixo está a função chamada **predict()** que prevê um valor de saída para uma linha a partir de um conjunto de coeficientes. Execute-a pois precisaremos dela futuramente neste tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(row, coefficients):\n",
    "  yhat = coefficients[0]\n",
    "  for i in range(len(row)-1):\n",
    "    yhat += coefficients[i + 1] * row[i]\n",
    "  return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O primeiro coeficiente é chamado de b0 ou viés, pois é autônomo e não é multiplicados por um valor de entrada (x) como os demais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar o seguinte conjunto simples de dados como dataset para testar nossa função:\n",
    "<br />\n",
    "**x**, **y** <br />\n",
    "1, 1 <br />\n",
    "2, 3 <br />\n",
    "4, 3 <br />\n",
    "3, 2 <br />\n",
    "5, 5 <br />\n",
    "<br />\n",
    "**coef**<br />\n",
    "0.4<br />\n",
    "0.8<br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos testar nossa função com esses valores da seguinte forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected=1.000, Predicted=1.200\n",
      "Expected=3.000, Predicted=2.000\n",
      "Expected=3.000, Predicted=3.600\n",
      "Expected=2.000, Predicted=2.800\n",
      "Expected=5.000, Predicted=4.400\n"
     ]
    }
   ],
   "source": [
    "#Exemplo predict() com coeficientes\n",
    "dataset = [[1, 1], [2, 3], [4, 3], [3, 2], [5, 5]]\n",
    "coef = [0.4, 0.8]\n",
    "\n",
    "for row in dataset:\n",
    "    yhat = predict(row, coef)\n",
    "    print(\"Expected=%.3f, Predicted=%.3f\" % (row[-1], yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Já tendo executado a função **predict()**, execute a célula acima para ver o resultado. Neste exemplo acima, para cada x há dois coeficientes (b0 = 0.4 e <br /> b1 = 0.8). A equação de previsão que modelamos para esse problema é:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**<center>y = b0 + b1 \\* x </center>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora estamos prontos para implementar um algoritmo de Regressão Linear usando Gradiente Descendente Estocástico para encontrar nossos coeficientes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />\n",
    "<br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Estimando coeficientes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos estimar os valores de coeficientes para nossos dados de treino usando Gradiente Descendente Estocástico. Esta operação requer dois parâmetros:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Taxa de aprendizado**: usado para limitar a quantidade que cada coeficiente é corrigido sempre que é atualizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Períodos**: o número de vezes para percorrer os dados de treinamento enquanto atualiza os coeficientes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Juntamente com os dados de treinamento, esses dois argumentos serão usados na função."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função executará 3 loops: loop por período, loop por linha em cada período, loop por cada coeficiente que \n",
    "o atualiza pra cada linha no período. Em resumo, atualizamos cada coeficiente para cada linha nos dados \n",
    "de treinamento, em cada período.\n",
    "\n",
    "Os coeficientes são atualizados baseados nos erros que o modelo fez."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br />\n",
    "O erro é calculado como a diferença entre a previsão feita com os coeficientes e o valor de saída esperado. <br />\n",
    "<br />\n",
    "<center>**erro = previsão - saída esperada**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />\n",
    "Há um coeficiente para cada x, e eles são atualizados de forma consistente, por exemplo: <br />\n",
    "<br />\n",
    "**<center>b1(t+1) = b1(t) - taxa de erro \\* erro(t) \\* x1(t)</center>**\n",
    "<br />\n",
    "<br />\n",
    "O primeiro coeficiente da lista é atualizado de forma similar, exceto que a fórmula não possui um x associado.\n",
    "<br />\n",
    "<br />\n",
    "**<center>b0(t+1) = b0(t) - taxa de erro * erro(t)</center>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora podemos juntar todas essas informações.<br />\n",
    "<br />\n",
    "<br />\n",
    "Abaixo está a função **coefficients_sgd()** que calcula os valores dos coeficientes de Regressão Linear para um conjunto de dados de treinamento usando Gradiente Descendente Estocástico. Execute-o para que possamos usá-lo posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def coefficients_sgd(train, l_rate, n_epoch):\n",
    "    coef = [0.0 for i in range(len(train[0]))]\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0\n",
    "        for row in train:\n",
    "            yhat = predict(row, coef)\n",
    "            error = yhat - row[-1]\n",
    "            sum_error += error**2\n",
    "            coef[0] = coef[0] - l_rate * error\n",
    "            for i in range(len(row)-1):\n",
    "                coef[i + 1] = coef[i + 1] - l_rate * error * row[i] \n",
    "                print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    "    return coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No código acima, além das operações previamente mencionadas, calculamos a soma do erro ao quadrado de cada período para que possamos imprimi-lo como mensagem no loop externo. Podemos testar esta função no nosso pequeno conjunto de dados já criado (dataset). Devemos criar, entretanto, a taxa de aprendizado e o número de períodos (l_rate e n_epoch, respectivamente encontrados abaixo). Entenda o código abaixo e veja qual o resultado de sua execução."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=0, lrate=0.001, error=1.000\n",
      ">epoch=0, lrate=0.001, error=9.982\n",
      ">epoch=0, lrate=0.001, error=18.791\n",
      ">epoch=0, lrate=0.001, error=22.541\n",
      ">epoch=0, lrate=0.001, error=46.236\n",
      ">epoch=1, lrate=0.001, error=0.878\n",
      ">epoch=1, lrate=0.001, error=9.204\n",
      ">epoch=1, lrate=0.001, error=16.819\n",
      ">epoch=1, lrate=0.001, error=19.985\n",
      ">epoch=1, lrate=0.001, error=41.305\n",
      ">epoch=2, lrate=0.001, error=0.771\n",
      ">epoch=2, lrate=0.001, error=8.501\n",
      ">epoch=2, lrate=0.001, error=15.070\n",
      ">epoch=2, lrate=0.001, error=17.732\n",
      ">epoch=2, lrate=0.001, error=36.930\n",
      ">epoch=3, lrate=0.001, error=0.676\n",
      ">epoch=3, lrate=0.001, error=7.865\n",
      ">epoch=3, lrate=0.001, error=13.520\n",
      ">epoch=3, lrate=0.001, error=15.746\n",
      ">epoch=3, lrate=0.001, error=33.047\n",
      ">epoch=4, lrate=0.001, error=0.593\n",
      ">epoch=4, lrate=0.001, error=7.290\n",
      ">epoch=4, lrate=0.001, error=12.146\n",
      ">epoch=4, lrate=0.001, error=13.998\n",
      ">epoch=4, lrate=0.001, error=29.601\n",
      ">epoch=5, lrate=0.001, error=0.519\n",
      ">epoch=5, lrate=0.001, error=6.769\n",
      ">epoch=5, lrate=0.001, error=10.929\n",
      ">epoch=5, lrate=0.001, error=12.459\n",
      ">epoch=5, lrate=0.001, error=26.543\n",
      ">epoch=6, lrate=0.001, error=0.454\n",
      ">epoch=6, lrate=0.001, error=6.297\n",
      ">epoch=6, lrate=0.001, error=9.849\n",
      ">epoch=6, lrate=0.001, error=11.105\n",
      ">epoch=6, lrate=0.001, error=23.830\n",
      ">epoch=7, lrate=0.001, error=0.397\n",
      ">epoch=7, lrate=0.001, error=5.868\n",
      ">epoch=7, lrate=0.001, error=8.893\n",
      ">epoch=7, lrate=0.001, error=9.914\n",
      ">epoch=7, lrate=0.001, error=21.422\n",
      ">epoch=8, lrate=0.001, error=0.346\n",
      ">epoch=8, lrate=0.001, error=5.479\n",
      ">epoch=8, lrate=0.001, error=8.045\n",
      ">epoch=8, lrate=0.001, error=8.868\n",
      ">epoch=8, lrate=0.001, error=19.285\n",
      ">epoch=9, lrate=0.001, error=0.302\n",
      ">epoch=9, lrate=0.001, error=5.126\n",
      ">epoch=9, lrate=0.001, error=7.294\n",
      ">epoch=9, lrate=0.001, error=7.950\n",
      ">epoch=9, lrate=0.001, error=17.389\n",
      ">epoch=10, lrate=0.001, error=0.263\n",
      ">epoch=10, lrate=0.001, error=4.805\n",
      ">epoch=10, lrate=0.001, error=6.629\n",
      ">epoch=10, lrate=0.001, error=7.145\n",
      ">epoch=10, lrate=0.001, error=15.706\n",
      ">epoch=11, lrate=0.001, error=0.229\n",
      ">epoch=11, lrate=0.001, error=4.512\n",
      ">epoch=11, lrate=0.001, error=6.040\n",
      ">epoch=11, lrate=0.001, error=6.439\n",
      ">epoch=11, lrate=0.001, error=14.213\n",
      ">epoch=12, lrate=0.001, error=0.199\n",
      ">epoch=12, lrate=0.001, error=4.246\n",
      ">epoch=12, lrate=0.001, error=5.518\n",
      ">epoch=12, lrate=0.001, error=5.821\n",
      ">epoch=12, lrate=0.001, error=12.888\n",
      ">epoch=13, lrate=0.001, error=0.172\n",
      ">epoch=13, lrate=0.001, error=4.003\n",
      ">epoch=13, lrate=0.001, error=5.056\n",
      ">epoch=13, lrate=0.001, error=5.280\n",
      ">epoch=13, lrate=0.001, error=11.712\n",
      ">epoch=14, lrate=0.001, error=0.149\n",
      ">epoch=14, lrate=0.001, error=3.781\n",
      ">epoch=14, lrate=0.001, error=4.646\n",
      ">epoch=14, lrate=0.001, error=4.807\n",
      ">epoch=14, lrate=0.001, error=10.668\n",
      ">epoch=15, lrate=0.001, error=0.129\n",
      ">epoch=15, lrate=0.001, error=3.579\n",
      ">epoch=15, lrate=0.001, error=4.284\n",
      ">epoch=15, lrate=0.001, error=4.395\n",
      ">epoch=15, lrate=0.001, error=9.742\n",
      ">epoch=16, lrate=0.001, error=0.111\n",
      ">epoch=16, lrate=0.001, error=3.394\n",
      ">epoch=16, lrate=0.001, error=3.963\n",
      ">epoch=16, lrate=0.001, error=4.035\n",
      ">epoch=16, lrate=0.001, error=8.921\n",
      ">epoch=17, lrate=0.001, error=0.095\n",
      ">epoch=17, lrate=0.001, error=3.224\n",
      ">epoch=17, lrate=0.001, error=3.679\n",
      ">epoch=17, lrate=0.001, error=3.722\n",
      ">epoch=17, lrate=0.001, error=8.191\n",
      ">epoch=18, lrate=0.001, error=0.082\n",
      ">epoch=18, lrate=0.001, error=3.069\n",
      ">epoch=18, lrate=0.001, error=3.428\n",
      ">epoch=18, lrate=0.001, error=3.451\n",
      ">epoch=18, lrate=0.001, error=7.544\n",
      ">epoch=19, lrate=0.001, error=0.070\n",
      ">epoch=19, lrate=0.001, error=2.927\n",
      ">epoch=19, lrate=0.001, error=3.205\n",
      ">epoch=19, lrate=0.001, error=3.215\n",
      ">epoch=19, lrate=0.001, error=6.970\n",
      ">epoch=20, lrate=0.001, error=0.060\n",
      ">epoch=20, lrate=0.001, error=2.796\n",
      ">epoch=20, lrate=0.001, error=3.008\n",
      ">epoch=20, lrate=0.001, error=3.011\n",
      ">epoch=20, lrate=0.001, error=6.461\n",
      ">epoch=21, lrate=0.001, error=0.051\n",
      ">epoch=21, lrate=0.001, error=2.676\n",
      ">epoch=21, lrate=0.001, error=2.834\n",
      ">epoch=21, lrate=0.001, error=2.834\n",
      ">epoch=21, lrate=0.001, error=6.009\n",
      ">epoch=22, lrate=0.001, error=0.043\n",
      ">epoch=22, lrate=0.001, error=2.566\n",
      ">epoch=22, lrate=0.001, error=2.681\n",
      ">epoch=22, lrate=0.001, error=2.683\n",
      ">epoch=22, lrate=0.001, error=5.607\n",
      ">epoch=23, lrate=0.001, error=0.036\n",
      ">epoch=23, lrate=0.001, error=2.465\n",
      ">epoch=23, lrate=0.001, error=2.544\n",
      ">epoch=23, lrate=0.001, error=2.552\n",
      ">epoch=23, lrate=0.001, error=5.251\n",
      ">epoch=24, lrate=0.001, error=0.030\n",
      ">epoch=24, lrate=0.001, error=2.372\n",
      ">epoch=24, lrate=0.001, error=2.424\n",
      ">epoch=24, lrate=0.001, error=2.440\n",
      ">epoch=24, lrate=0.001, error=4.935\n",
      ">epoch=25, lrate=0.001, error=0.025\n",
      ">epoch=25, lrate=0.001, error=2.286\n",
      ">epoch=25, lrate=0.001, error=2.318\n",
      ">epoch=25, lrate=0.001, error=2.345\n",
      ">epoch=25, lrate=0.001, error=4.655\n",
      ">epoch=26, lrate=0.001, error=0.021\n",
      ">epoch=26, lrate=0.001, error=2.207\n",
      ">epoch=26, lrate=0.001, error=2.224\n",
      ">epoch=26, lrate=0.001, error=2.264\n",
      ">epoch=26, lrate=0.001, error=4.406\n",
      ">epoch=27, lrate=0.001, error=0.017\n",
      ">epoch=27, lrate=0.001, error=2.133\n",
      ">epoch=27, lrate=0.001, error=2.141\n",
      ">epoch=27, lrate=0.001, error=2.196\n",
      ">epoch=27, lrate=0.001, error=4.186\n",
      ">epoch=28, lrate=0.001, error=0.014\n",
      ">epoch=28, lrate=0.001, error=2.066\n",
      ">epoch=28, lrate=0.001, error=2.068\n",
      ">epoch=28, lrate=0.001, error=2.138\n",
      ">epoch=28, lrate=0.001, error=3.990\n",
      ">epoch=29, lrate=0.001, error=0.011\n",
      ">epoch=29, lrate=0.001, error=2.003\n",
      ">epoch=29, lrate=0.001, error=2.003\n",
      ">epoch=29, lrate=0.001, error=2.090\n",
      ">epoch=29, lrate=0.001, error=3.816\n",
      ">epoch=30, lrate=0.001, error=0.009\n",
      ">epoch=30, lrate=0.001, error=1.946\n",
      ">epoch=30, lrate=0.001, error=1.946\n",
      ">epoch=30, lrate=0.001, error=2.050\n",
      ">epoch=30, lrate=0.001, error=3.662\n",
      ">epoch=31, lrate=0.001, error=0.007\n",
      ">epoch=31, lrate=0.001, error=1.892\n",
      ">epoch=31, lrate=0.001, error=1.896\n",
      ">epoch=31, lrate=0.001, error=2.017\n",
      ">epoch=31, lrate=0.001, error=3.525\n",
      ">epoch=32, lrate=0.001, error=0.006\n",
      ">epoch=32, lrate=0.001, error=1.843\n",
      ">epoch=32, lrate=0.001, error=1.852\n",
      ">epoch=32, lrate=0.001, error=1.991\n",
      ">epoch=32, lrate=0.001, error=3.404\n",
      ">epoch=33, lrate=0.001, error=0.004\n",
      ">epoch=33, lrate=0.001, error=1.797\n",
      ">epoch=33, lrate=0.001, error=1.813\n",
      ">epoch=33, lrate=0.001, error=1.970\n",
      ">epoch=33, lrate=0.001, error=3.296\n",
      ">epoch=34, lrate=0.001, error=0.003\n",
      ">epoch=34, lrate=0.001, error=1.754\n",
      ">epoch=34, lrate=0.001, error=1.779\n",
      ">epoch=34, lrate=0.001, error=1.953\n",
      ">epoch=34, lrate=0.001, error=3.200\n",
      ">epoch=35, lrate=0.001, error=0.002\n",
      ">epoch=35, lrate=0.001, error=1.714\n",
      ">epoch=35, lrate=0.001, error=1.748\n",
      ">epoch=35, lrate=0.001, error=1.941\n",
      ">epoch=35, lrate=0.001, error=3.115\n",
      ">epoch=36, lrate=0.001, error=0.002\n",
      ">epoch=36, lrate=0.001, error=1.678\n",
      ">epoch=36, lrate=0.001, error=1.722\n",
      ">epoch=36, lrate=0.001, error=1.932\n",
      ">epoch=36, lrate=0.001, error=3.040\n",
      ">epoch=37, lrate=0.001, error=0.001\n",
      ">epoch=37, lrate=0.001, error=1.643\n",
      ">epoch=37, lrate=0.001, error=1.698\n",
      ">epoch=37, lrate=0.001, error=1.925\n",
      ">epoch=37, lrate=0.001, error=2.973\n",
      ">epoch=38, lrate=0.001, error=0.001\n",
      ">epoch=38, lrate=0.001, error=1.612\n",
      ">epoch=38, lrate=0.001, error=1.678\n",
      ">epoch=38, lrate=0.001, error=1.921\n",
      ">epoch=38, lrate=0.001, error=2.914\n",
      ">epoch=39, lrate=0.001, error=0.000\n",
      ">epoch=39, lrate=0.001, error=1.582\n",
      ">epoch=39, lrate=0.001, error=1.660\n",
      ">epoch=39, lrate=0.001, error=1.920\n",
      ">epoch=39, lrate=0.001, error=2.862\n",
      ">epoch=40, lrate=0.001, error=0.000\n",
      ">epoch=40, lrate=0.001, error=1.554\n",
      ">epoch=40, lrate=0.001, error=1.644\n",
      ">epoch=40, lrate=0.001, error=1.920\n",
      ">epoch=40, lrate=0.001, error=2.815\n",
      ">epoch=41, lrate=0.001, error=0.000\n",
      ">epoch=41, lrate=0.001, error=1.529\n",
      ">epoch=41, lrate=0.001, error=1.630\n",
      ">epoch=41, lrate=0.001, error=1.921\n",
      ">epoch=41, lrate=0.001, error=2.773\n",
      ">epoch=42, lrate=0.001, error=0.000\n",
      ">epoch=42, lrate=0.001, error=1.505\n",
      ">epoch=42, lrate=0.001, error=1.618\n",
      ">epoch=42, lrate=0.001, error=1.924\n",
      ">epoch=42, lrate=0.001, error=2.737\n",
      ">epoch=43, lrate=0.001, error=0.000\n",
      ">epoch=43, lrate=0.001, error=1.482\n",
      ">epoch=43, lrate=0.001, error=1.607\n",
      ">epoch=43, lrate=0.001, error=1.928\n",
      ">epoch=43, lrate=0.001, error=2.704\n",
      ">epoch=44, lrate=0.001, error=0.000\n",
      ">epoch=44, lrate=0.001, error=1.462\n",
      ">epoch=44, lrate=0.001, error=1.598\n",
      ">epoch=44, lrate=0.001, error=1.932\n",
      ">epoch=44, lrate=0.001, error=2.675\n",
      ">epoch=45, lrate=0.001, error=0.000\n",
      ">epoch=45, lrate=0.001, error=1.442\n",
      ">epoch=45, lrate=0.001, error=1.590\n",
      ">epoch=45, lrate=0.001, error=1.937\n",
      ">epoch=45, lrate=0.001, error=2.650\n",
      ">epoch=46, lrate=0.001, error=0.000\n",
      ">epoch=46, lrate=0.001, error=1.424\n",
      ">epoch=46, lrate=0.001, error=1.582\n",
      ">epoch=46, lrate=0.001, error=1.943\n",
      ">epoch=46, lrate=0.001, error=2.627\n",
      ">epoch=47, lrate=0.001, error=0.000\n",
      ">epoch=47, lrate=0.001, error=1.407\n",
      ">epoch=47, lrate=0.001, error=1.576\n",
      ">epoch=47, lrate=0.001, error=1.949\n",
      ">epoch=47, lrate=0.001, error=2.607\n",
      ">epoch=48, lrate=0.001, error=0.001\n",
      ">epoch=48, lrate=0.001, error=1.391\n",
      ">epoch=48, lrate=0.001, error=1.571\n",
      ">epoch=48, lrate=0.001, error=1.955\n",
      ">epoch=48, lrate=0.001, error=2.589\n",
      ">epoch=49, lrate=0.001, error=0.001\n",
      ">epoch=49, lrate=0.001, error=1.376\n",
      ">epoch=49, lrate=0.001, error=1.566\n",
      ">epoch=49, lrate=0.001, error=1.962\n",
      ">epoch=49, lrate=0.001, error=2.573\n",
      "[0.22998234937311363, 0.8017220304137576]\n"
     ]
    }
   ],
   "source": [
    "l_rate = 0.001\n",
    "n_epoch = 50\n",
    "coef = coefficients_sgd(dataset, l_rate, n_epoch)\n",
    "print(coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que o erro vai decaindo até o final do período para cada linha.\n",
    "Observe que poderíamos treinar por muito mais tempo (mais períodos) ou aumentar a quantidade que atualizamos os \n",
    "coeficientes de cada período (taxa de aprendizado mais alta).\n",
    "<br />\n",
    "<br />\n",
    "Sinta-se à vontade para testar as funções acima alterando os valores de entrada das funções."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Estudo de caso do vinho de qualidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos testar o nosso modelo de Regressão Linear usando Gradiente Descendente Estocástico em um dataset de qualidade de vinho. O exemplo que utilizaremos assume que uma cópia CSV do dataset já está no seu diretório de trabalho com o nome **winequality-white.csv**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para trabalhar com os dados carregamos o dataset, os valores strings são convertidos para numéricos float e cada coluna\n",
    "tem seus valores normalizados para valores no range de 0 a 1. Esses passos são efetuados com as funções auxiliares \n",
    "loas_csv() e str_column_to_float() para carregar e preparar o dataset, e dataset_minmax() e normalize_dataset() \n",
    "para normalizá-lo.\n",
    "\n",
    "Usaremos a validação cruzada de k partes para estimar o desempenho do modelo aprendido em dados não vistos. \n",
    "Isso significa que vamos construir e avaliar k modelos e estimar o desempenho como o erro médio do modelo. \n",
    "O erro quadrático médio da raiz será usado para avaliar cada modelo. Esses comportamentos são fornecidos nas funções \n",
    "cross_validation_split(), rmse_metric() e evaluate_algorithm().\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos as funções **predict()**, **coefficients_sgd()** e **linear_regression_sgd()** para treinar o modelo.\n",
    "Abaixo encontra-se o exemplo completo, analise-o e execute-o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.12248058224159092, 0.13034017509167112, 0.12620370547483578, 0.12897687952843237, 0.12446990678682233]\n",
      "Mean RMSE: 0.126\n"
     ]
    }
   ],
   "source": [
    "#Importar funções necessárias\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "from math import sqrt\n",
    "\n",
    "#Carregar arquivo CSV\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "#Converter colunas string para float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    "\n",
    "#Encontrar valores min e max para cada coluna\n",
    "def dataset_minmax(dataset):\n",
    "    minmax = list()\n",
    "    for i in range(len(dataset[0])):\n",
    "        col_values = [row[i] for row in dataset]\n",
    "        value_min = min(col_values)\n",
    "        value_max = max(col_values)\n",
    "        minmax.append([value_min, value_max])\n",
    "    return minmax\n",
    "\n",
    "#Reescalar colunas do dataset para o range 0-1\n",
    "def normalize_dataset(dataset, minmax):\n",
    "    for row in dataset:\n",
    "        for i in range(len(row)):\n",
    "             row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    "\n",
    "#Dividir o dataset em k partes\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for i in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    "\n",
    "#Calcular a média do erro ao quadrado\n",
    "def rmse_metric(actual, predicted):\n",
    "    sum_error = 0.0\n",
    "    for i in range(len(actual)):\n",
    "        prediction_error = predicted[i] - actual[i]\n",
    "        sum_error += (prediction_error ** 2)\n",
    "    mean_error = sum_error / float(len(actual))\n",
    "    return sqrt(mean_error)\n",
    "\n",
    "#Avaliar um algoritmo usando uma divisão de validação cruzada\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        rmse = rmse_metric(actual, predicted)\n",
    "        scores.append(rmse)\n",
    "    return scores\n",
    "\n",
    "#Fazer previsões com coeficientes\n",
    "def predict(row, coefficients):\n",
    "    yhat = coefficients[0]\n",
    "    for i in range(len(row)-1):\n",
    "        yhat += coefficients[i + 1] * row[i]\n",
    "    return yhat\n",
    "\n",
    "#Estimar coeficientes de Regressão Linear usando Gradiente Descendente Estocástico\n",
    "def coefficients_sgd(train, l_rate, n_epoch):\n",
    "    coef = [0.0 for i in range(len(train[0]))]\n",
    "    for epoch in range(n_epoch):\n",
    "        for row in train:\n",
    "            yhat = predict(row, coef)\n",
    "            error = yhat - row[-1]\n",
    "            coef[0] = coef[0] - l_rate * error\n",
    "            for i in range(len(row)-1):\n",
    "                coef[i + 1] = coef[i + 1] - l_rate * error * row[i]\n",
    "            # print(l_rate, n_epoch, error)\n",
    "    return coef\n",
    "\n",
    "#Algoritmo de Regressão Lenar com Gradiente Descendente Estocástico\n",
    "def linear_regression_sgd(train, test, l_rate, n_epoch):\n",
    "    predictions = list()\n",
    "    coef = coefficients_sgd(train, l_rate, n_epoch)\n",
    "    for row in test:\n",
    "        yhat = predict(row, coef)\n",
    "        predictions.append(yhat)\n",
    "    return(predictions)\n",
    "\n",
    "\n",
    "\n",
    "#Regressão Linear com dataset de qualidade de vinho\n",
    "seed(1)\n",
    "\n",
    "# carregando e preparandos dados\n",
    "filename = 'winequality-white.csv' \n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])):\n",
    "    str_column_to_float(dataset, i)\n",
    "    \n",
    "# normalizar\n",
    "minmax = dataset_minmax(dataset)\n",
    "normalize_dataset(dataset, minmax)\n",
    "\n",
    "# testar algoritmo\n",
    "n_folds = 5\n",
    "l_rate = 0.01\n",
    "n_epoch = 50\n",
    "scores = evaluate_algorithm(dataset, linear_regression_sgd, n_folds, l_rate, n_epoch) \n",
    "print('Scores: %s' % scores)\n",
    "print('Mean RMSE: %.3f' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O k = 5 foi usado para fazer validação cruzada, dando cada parte 4898/5 = 979.6, ou seja, menos de 1000 registros \n",
    "avaliados para cada iteração. Como podes observar, foi escolhida uma taxa e aprendizado igual a 0.01 e 50 períodos de treino. No fim do treino, o programa mostra a média dos scores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
